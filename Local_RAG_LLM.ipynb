{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) from PDFs with Local Vector Storage (ChromaDB), Enhanced Reflection, and Iterative Self-Correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mermaid Flowchart (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import ollama\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding models\n",
    "JSONparser = JsonOutputParser()\n",
    "STRparser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding function\n",
    "def get_embedding(text):\n",
    "    response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=text)\n",
    "    return response[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: H1 Residential - Large Lot Zone.pdf\n",
      "All 1 files processed successfully.\n",
      "Total number of chunks loaded: 31\n"
     ]
    }
   ],
   "source": [
    "# Process PDF files\n",
    "def op2_process_pdf_files(folder_path, prefix, suffix):\n",
    "    chunks = []\n",
    "    pdf_files = [file for file in os.listdir(folder_path) if file.startswith(prefix) and file.endswith(suffix)]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        document = loader.load()\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=80,\n",
    "            length_function=len\n",
    "        )\n",
    "        document = text_splitter.split_documents(document)\n",
    "        \n",
    "        chunks.extend(document)\n",
    "        print(f\"Processed: {pdf_file}\")\n",
    "        \n",
    "    print(f\"All {len(pdf_files)} files processed successfully.\")\n",
    "    print(f\"Total number of chunks loaded: {len(chunks)}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "folder_path = r\"C:\\Users\\GGPC\\Desktop\\Work\\RAG_Local_LLM\\data\"\n",
    "prefix = \"H1 Residential\"\n",
    "suffix = \".pdf\"\n",
    "\n",
    "chunks = op2_process_pdf_files(folder_path, prefix, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_ids(chunks):\n",
    "    # Id format: Page Source : Page Number : Chunk Index\n",
    "    \n",
    "    chunk_counter = {}\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "        \n",
    "        # chunk index counter\n",
    "        if current_page_id not in chunk_counter:\n",
    "            chunk_counter[current_page_id] = 0\n",
    "        else:\n",
    "            chunk_counter[current_page_id] += 1\n",
    "\n",
    "        # calculate chunk id\n",
    "        chunk_id = f\"{current_page_id}:{chunk_counter[current_page_id]}\"\n",
    "        \n",
    "        # update chunk metadata\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Get chunk ids\n",
    "chunks = get_chunk_ids(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chroma client\n",
    "CHROMA_PATH = \"chroma_db2\"\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create a collection\n",
    "collection_name = \"docs\"\n",
    "collection = client.get_or_create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No new documents to add\n",
      "Total documents in DB: 31\n"
     ]
    }
   ],
   "source": [
    "# Add documents to the collection\n",
    "existing_ids = set(collection.get()[\"ids\"])\n",
    "new_chunks = [chunk for chunk in chunks if chunk.metadata.get(\"id\") not in existing_ids]\n",
    "\n",
    "if new_chunks:\n",
    "    print(f\"ðŸ‘‰ Adding new documents: {len(new_chunks)}\")\n",
    "    for chunk in new_chunks:\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "        collection.add(\n",
    "            ids=[chunk.metadata[\"id\"]],\n",
    "            embeddings=[embedding],\n",
    "            documents=[chunk.page_content],\n",
    "            metadatas=[chunk.metadata]\n",
    "        )\n",
    "else:\n",
    "    print(\"âœ… No new documents to add\")\n",
    "\n",
    "print(f\"Total documents in DB: {len(collection.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaEmbeddingFunction:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        response = ollama.embeddings(model=self.model_name, prompt=text)\n",
    "        return response[\"embedding\"]\n",
    "\n",
    "# Use this wrapper class when creating the vectorstore\n",
    "embedding_function = ChromaEmbeddingFunction(\"mxbai-embed-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorstore and retriever\n",
    "vectorstore = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:1:1', 'page': 1, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='(b) are compatible with the scale and intensity of development anticipated \\nwithin the zone; and  \\n(c) avoid, remedy or mitigate adverse effects on residential amenity; and  \\n(d) will not detract from the vitality of the Business â€“ City Centre  Zone , \\nBusiness â€“ Metro politan  Centre Zone and the Business â€“ Town \\nCentre Zone.  \\nH1.4.  Activity table  \\nTable H1.4.1 Activity t able specifies the activity status of land use and development  \\nactivities in the Residential â€“ Large Lot Zone pursuant to section 9(3) of the Resource \\nManagement Act 1991.    \\nTable  H1.4.1 Activity table \\nActivity  Activity \\nstatus  Standards to be complied with  \\nUse \\n(A1) Activities not provided for  NC  \\nResidential  \\n(A2) Camping grounds  D  \\n(A3) One dwelling per site  P Standard H1.6.4 Building height;'),\n",
       " Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:9:2', 'page': 9, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='with the surrounding residential area .  \\n(c) traffic:  \\n(i) whether the activity avoids or mitigates high levels of additional \\nnon-residential traffic on local roads.  \\n(d) location and design of parking and access :  \\n(i) whether access is provided or required.  \\n(e) noise, lighting  and hours of operation : \\n(i) whether n oise and lighting and the hours of operation of the \\nactivity avoids, remedies or mitigates adverse effects on the \\nresidential amenity of surrounding properties, by:  \\nâ€¢ locating noisy activities away from neighbouring residential \\nboundaries; and'),\n",
       " Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:8:0', 'page': 8, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='H1 Residential â€“ Large Lot Zone  \\n \\nAuckland Unitary Plan Operative in part   9 Note: Building Act regulations apply. A building consent may be required under the \\nBuilding Act.  \\nH1.7.  Assessment  â€“ controlled activities  \\nThere are no controlled activities in this zone . \\nH1.8.  Assessment â€“ restricted discretionary a ctivities  \\nH1.8.1.  Matters of discretion  \\nThe C ouncil will restrict its discretion to all the following matters when assessing a \\nrestricted discretionary activity resource consent application:  \\n(1) for supported residential care accommodating up to 10 people per site inclusive \\nof staff and residents; boarding houses accommodating up to 10 people per site \\ninclusive of staff and residents; and visitor accommodation accommodating up to'),\n",
       " Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:4:0', 'page': 4, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='H1 Residential â€“ Large Lot Zone  \\n \\nAuckland Unitary Plan Operative in part   5 H1.6.  Standards  \\nH1.6.1.  Activities l isted in Table H1.4.1 Activity table  \\n(1) Activities and buildings containing activities  listed in Table H1.4.1 Activity table \\nmust comply with the standards listed in the column in Table H1.4.1 called  \\nStandards to be complied with.  \\nH1.6.2.  Home occupations  \\nPurpose: to enable people to work from home at a scale that the residential character \\nand amenity is maintained.  \\n(1) A home occupation must comply with all the following standards : \\n(a) at least one person engaged in the home occupation must use the \\ndwelling on the  site as their principal place of residence ; \\n(b) no more than two people who do not use the dwelling as their principal')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.invoke(\"what are the permitted activities?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the models and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:1:1', 'page': 1, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='(b) are compatible with the scale and intensity of development anticipated \\nwithin the zone; and  \\n(c) avoid, remedy or mitigate adverse effects on residential amenity; and  \\n(d) will not detract from the vitality of the Business â€“ City Centre  Zone , \\nBusiness â€“ Metro politan  Centre Zone and the Business â€“ Town \\nCentre Zone.  \\nH1.4.  Activity table  \\nTable H1.4.1 Activity t able specifies the activity status of land use and development  \\nactivities in the Residential â€“ Large Lot Zone pursuant to section 9(3) of the Resource \\nManagement Act 1991.    \\nTable  H1.4.1 Activity table \\nActivity  Activity \\nstatus  Standards to be complied with  \\nUse \\n(A1) Activities not provided for  NC  \\nResidential  \\n(A2) Camping grounds  D  \\n(A3) One dwelling per site  P Standard H1.6.4 Building height;'),\n",
       "  Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:9:2', 'page': 9, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='with the surrounding residential area .  \\n(c) traffic:  \\n(i) whether the activity avoids or mitigates high levels of additional \\nnon-residential traffic on local roads.  \\n(d) location and design of parking and access :  \\n(i) whether access is provided or required.  \\n(e) noise, lighting  and hours of operation : \\n(i) whether n oise and lighting and the hours of operation of the \\nactivity avoids, remedies or mitigates adverse effects on the \\nresidential amenity of surrounding properties, by:  \\nâ€¢ locating noisy activities away from neighbouring residential \\nboundaries; and'),\n",
       "  Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:8:0', 'page': 8, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='H1 Residential â€“ Large Lot Zone  \\n \\nAuckland Unitary Plan Operative in part   9 Note: Building Act regulations apply. A building consent may be required under the \\nBuilding Act.  \\nH1.7.  Assessment  â€“ controlled activities  \\nThere are no controlled activities in this zone . \\nH1.8.  Assessment â€“ restricted discretionary a ctivities  \\nH1.8.1.  Matters of discretion  \\nThe C ouncil will restrict its discretion to all the following matters when assessing a \\nrestricted discretionary activity resource consent application:  \\n(1) for supported residential care accommodating up to 10 people per site inclusive \\nof staff and residents; boarding houses accommodating up to 10 people per site \\ninclusive of staff and residents; and visitor accommodation accommodating up to'),\n",
       "  Document(metadata={'id': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf:4:0', 'page': 4, 'source': 'C:\\\\Users\\\\GGPC\\\\Desktop\\\\Work\\\\RAG_Local_LLM\\\\data\\\\H1 Residential - Large Lot Zone.pdf'}, page_content='H1 Residential â€“ Large Lot Zone  \\n \\nAuckland Unitary Plan Operative in part   5 H1.6.  Standards  \\nH1.6.1.  Activities l isted in Table H1.4.1 Activity table  \\n(1) Activities and buildings containing activities  listed in Table H1.4.1 Activity table \\nmust comply with the standards listed in the column in Table H1.4.1 called  \\nStandards to be complied with.  \\nH1.6.2.  Home occupations  \\nPurpose: to enable people to work from home at a scale that the residential character \\nand amenity is maintained.  \\n(1) A home occupation must comply with all the following standards : \\n(a) at least one person engaged in the home occupation must use the \\ndwelling on the  site as their principal place of residence ; \\n(b) no more than two people who do not use the dwelling as their principal')],\n",
       " 'question': 'what are the permitted activities?'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the 'setup' part of chain\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "setup.invoke(\"what are the permitted activities?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grading\n",
    "\n",
    "# Local Model\n",
    "MODEL = \"llama3.1\"\n",
    "model = Ollama(model=MODEL, format=\"json\", temperature=0)# temperature 0 for more accurate / consistnat answers\n",
    "\n",
    "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a grader assessing the relevance of a retrieved document to a user question.\\n\n",
    "Your task is to determine if the document contains any keywords or concepts that directly relate to the user's question.\\n\n",
    "This is a broad relevance test, and you do not need to assess the accuracy of the document's contentâ€”only whether the document has related keywords or ideas.\\n\n",
    "\n",
    "If the document contains words or concepts that are clearly relevant to the user question, return \"yes\".\\n\n",
    "If the document lacks such keywords or is off-topic, return \"no\".\\n\n",
    "\n",
    "Provide your decision as a JSON object with a single key \"score\" and a binary value (\"yes\" or \"no\"). Do not provide any explanation or preambleâ€”just the JSON output.\n",
    "<|end_of_text_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here is the retrieved document: {document}\\n\\n\n",
    "Here is the user question: {question}\\n\\n\n",
    "<|end_of_text_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "ret_prompt = PromptTemplate.from_template(template, input=[\"document\", \"question\"])\n",
    "retrieval_grader = ret_prompt | model | JSONparser\n",
    "\n",
    "question = \"what are the permitted activities?\"\n",
    "documents = retriever.invoke(question)\n",
    "text = [doc.page_content for doc in documents]\n",
    "\n",
    "print(retrieval_grader.invoke({\"document\": text, \"question\": question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Permitted Activities in H1 Residential - Large Lot Zone**\n",
      "\n",
      "Based on the provided context, the following activities are permitted in the H1 Residential - Large Lot Zone:\n",
      "\n",
      "* **Residential use**: This is explicitly mentioned as a permitted activity (Table H1.4.1 Activity table, A3: One dwelling per site).\n",
      "* **Home occupations**: As long as they comply with the standards listed in H1.6.2, home occupations are allowed. The standards include:\n",
      "\t+ At least one person engaged in the home occupation must use the dwelling on the site as their principal place of residence.\n",
      "\t+ No more than two people who do not use the dwelling as their principal place of residence may be employed or work from the dwelling.\n",
      "\n",
      "**Source:** Auckland Unitary Plan Operative in part, H1 Residential - Large Lot Zone documents.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "# Local Model\n",
    "MODEL = \"llama3.1\"\n",
    "model = Ollama(model=MODEL, temperature=0)# temperature 0 for more accurate / consistnat answers\n",
    "\n",
    "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Extract and provide detailed information from the context to answer the question.\\n\n",
    "Steps:\\n\n",
    "1. Analyze the provided context in detail.\\n\n",
    "2. Answer the question comprehensively, using relevant information from the context.\\n\n",
    "3. Structure your answer clearly, with appropriate headings, bullet points, or numbered lists if necessary.\\n\n",
    "4. State the source of the information.\\n\n",
    "5. If the answer is not present in the context, respond with: \"I couldn't find any relevant information in the provided context.\"\\n\\n\n",
    "<|end_of_text_id|><|start_header_id|>user<|end_header_id|>\n",
    "Context: {context}\\n\n",
    "Question: {question}\\n\\n\n",
    "Answer: <|end_of_text_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(template, input=[\"context\", \"question\"])\n",
    "rag_chain = rag_prompt | model | STRparser\n",
    "\n",
    "question = \"what are the permitted activities?\"\n",
    "documents = retriever.invoke(question)\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": \"yes\"}\n"
     ]
    }
   ],
   "source": [
    "### Hallucination Checker\n",
    "\n",
    "# Local Model\n",
    "MODEL = \"llama3.1\"\n",
    "model = Ollama(model=MODEL, format=\"json\", temperature=0)# temperature 0 for more accurate / consistnat answers\n",
    "\n",
    "template =\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader tasked with checking \n",
    "whether a generated answer contains hallucinations or fabricated information that is factually incorrect.\\n\n",
    "Your task is to assess the factual accuracy of the answer in relation to the question. \\n\n",
    "If the answer appears to contain hallucinations or incorrect information, grade it as 'no'. If the answer appears factually correct, grade it as 'yes'. \\n\n",
    "Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|> \n",
    "Here are the facts:\n",
    "\\n ------- \\n\n",
    "{document} \n",
    "\\n ------- \\n\n",
    "Here is the generated answer: {generation} \n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "\n",
    "hal_prompt = PromptTemplate.from_template(template, input=[\"document\", \"generation\"])\n",
    "hal_checker = hal_prompt | model | STRparser\n",
    "\n",
    "print(hal_checker.invoke({\"document\": documents, \"generation\": generation}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reflection': 'complete', 'missing_info': 'none'}\n"
     ]
    }
   ],
   "source": [
    "### Reflection Process for Generation and Retrieved Documents\n",
    "\n",
    "# Local Model\n",
    "MODEL = \"llama3.1\"\n",
    "model = Ollama(model=MODEL, format=\"json\", temperature=0)# temperature 0 for more accurate / consistnat answers\n",
    "\n",
    "# Prompt\n",
    "\n",
    "template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are tasked with reflecting on a generated answer \n",
    "and comparing it with the retrieved documents to check if any additional relevant information is missing.\\n\n",
    "Review the generated answer, compare it with the documents, and suggest if any more information should be added from the documents.\\n\n",
    "\n",
    "Provide the result as a JSON object with:\n",
    "- 'reflection': whether the answer is complete or missing important information.\\n\n",
    "- 'missing_info': additional details from the documents to improve the answer, or 'none' if the answer is complete.\\n\\n\n",
    "\n",
    "No additional explanation is required.\\n\\n <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Here is the generated answer:\n",
    "\\n ------- \\n\n",
    "{generation} \n",
    "\\n ------- \\n\n",
    "Here are the retrieved documents:\n",
    "\\n ------- \\n\n",
    "{document}\n",
    "\\n ------- \\n\n",
    "Here is the user question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "\n",
    "ref_prompt = PromptTemplate.from_template(template, input=[\"generation\", \"document\", \"question\"])\n",
    "reflector = ref_prompt | model | JSONparser\n",
    "\n",
    "print(reflector.invoke({\"generation\": generation, \"document\": documents, \"question\": question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Indicator\n",
    "\n",
    "# Local Model\n",
    "MODEL = \"llama3.1\"\n",
    "model = Ollama(model=MODEL, format=\"json\", temperature=0)# temperature 0 for more accurate / consistnat answers\n",
    "\n",
    "template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "user question to a vectorstore or web search.\\n\n",
    "Use the vectorstore for questions on zoning, land use, building controls, or environmental regulations in Auckland.\\n \n",
    "You do not need to be stringent with the keywords in the question related to these topics.\\n\n",
    "Otherwise, use web-search.\\n\n",
    "Give a binary choice 'web_search' or 'vectorstore' based on the question.\\n \n",
    "Return the a JSON with a single key 'datasource' and no premable or explanation.\\n\\n\n",
    "\n",
    "Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "\n",
    "ind_prompt = PromptTemplate.from_template(template, input=[\"question\"])\n",
    "indicator = ind_prompt | model | JSONparser\n",
    "question = \"what are the permitted activities?\"\n",
    "documents = retriever.get_relevant_documents(question)\n",
    "text = [doc.page_content for doc in documents]\n",
    "print(indicator.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Web_search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for setting up Control Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "### State Graph\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of graph.\n",
    "    \n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    \n",
    "    \n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents from the vector store.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the graph.\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"RETRIEVING...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    return{\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "    \n",
    "    Args: \n",
    "        state (dict): The current state of the graph.\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): New Key added to state, generation, that contains the LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"GENERATING...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG Generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_relevance(state):\n",
    "    \"\"\"\n",
    "    Determines if the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, flag to run web search\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): New key added to state, \n",
    "        \"\"\"\n",
    "        \n",
    "    print(\"GRADING RELEVANCE...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score documents\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for doc in documents:\n",
    "        score = retrieval_grader.invoke({\"document\": doc.page_content, \"question\": question})\n",
    "        grade = score[\"score\"]\n",
    "        \n",
    "        # If document is relevant, append to filtered_docs\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"DOCUMENT IS RELEVANT!\")\n",
    "            filtered_docs.append(doc)\n",
    "        # If document is irrelevant, set web_search to \"Yes\" and continue\n",
    "        else:\n",
    "            print(\"DOCUMENT IS IRRELEVANT!\")\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the quesion\n",
    "    \n",
    "    Args:\n",
    "        state (dict): state of the current graph\n",
    "        \n",
    "    Return:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"WEB SEARCHING...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Invoke web search tool\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([doc[\"content\"] for doc in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    \n",
    "    # Create documents list with web results\n",
    "    documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def reflect(state):\n",
    "    \"\"\"\n",
    "    Reflect on the generated answer and compare it with the retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the graph.\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): Updated state with reflection results.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"REFLECTING...\")\n",
    "    \n",
    "    generation = state[\"generation\"]\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    reflection = reflector.invoke({\"generation\": generation, \"document\": documents, \"question\": question})\n",
    "    ref_status = reflection[\"reflection\"]\n",
    "    missing_info = reflection[\"missing_info\"]\n",
    "    \n",
    "    if ref_status.lower() == \"complete\":\n",
    "        print(\"REFLECTION IS COMPLETE!\")\n",
    "        return {\"documents\": documents, \"question\": question, \"generation\": generation, \"reflection_status\": \"complete\"}\n",
    "    else:\n",
    "        print(\"MISSING INFORMATION IN GENERATION!\")\n",
    "        \n",
    "        updated_documents = documents.copy()\n",
    "        updated_documents.append({\"page_content\": missing_info})\n",
    "\n",
    "        return {\n",
    "            \"documents\": updated_documents,\n",
    "            \"question\": question,\n",
    "            \"generation\": generation,\n",
    "            \"reflection_status\": \"incomplete\"\n",
    "        }\n",
    "    \n",
    "### Conditional Edge\n",
    "\n",
    "\n",
    "def indication_question(state):\n",
    "    \"\"\"\n",
    "    Route question to RAG or web search.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the graph.\n",
    "        \n",
    "    Returns:\n",
    "        str :Next node to call\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"CHECKING IF VECTORSTORE IS SUITABLE...\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = indicator.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "    if source[\"datasource\"].lower() == \"web_search\":\n",
    "        print(\"ROUTING TO WEB SEARCH\")\n",
    "        return \"web_search\"\n",
    "    elif source[\"datasource\"].lower() == \"vectorstore\":\n",
    "        print(\"ROUTING TO VECTORSTORE\")\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "\n",
    "def generate_or_websearch(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the graph.\n",
    "        \n",
    "    Returns:\n",
    "        str: binary choice for next node call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"DETERMINING NEXT NODE...\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "    \n",
    "    if web_search.lower() == \"yes\":\n",
    "        print(\"FURTHER INFORMATION IS REQUIRED! ADDING WEB SEARCH RESULTS...\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"SUFFICIENT INFORMATION! GENERATING ANSWER...\")\n",
    "        return \"generate\"\n",
    "        \n",
    "\n",
    "### Conditional Edge\n",
    "\n",
    "def check_hallucination(state):\n",
    "    \"\"\"\n",
    "    Check if the generated answer contains hallucinations.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the graph.\n",
    "    \n",
    "    Returns:\n",
    "        str: decision for next node call\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"CHECKING FOR HALLUCINATION...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    score = hal_checker.invoke({\"document\": documents, \"generation\": generation})\n",
    "    grade = score[\"score\"]\n",
    "    \n",
    "    # checking hallucination\n",
    "    if grade.lower() == \"yes\":\n",
    "        print(\"GENERATION IS GROUNDED IN FACTS!\")\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        print(\"GENERATION CONTAINS HALLUCINATION!\")\n",
    "        return \"not_useful\"\n",
    "    \n",
    "\n",
    "\n",
    "### Creating workflow\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define nodes\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_relevance\", grade_relevance)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"reflect\", reflect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edges\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    indication_question,\n",
    "    {\n",
    "        \"web_search\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"grade_relevance\")\n",
    "workflow.add_edge(\"websearch\", \"grade_relevance\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_relevance\",\n",
    "    generate_or_websearch,\n",
    "    {\n",
    "        \"web_search\": \"websearch\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"generate\", \"reflect\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "    lambda x: \"complete\" if x[\"reflection_status\"] == \"complete\" else \"incomplete\",\n",
    "    {\n",
    "        \"complete\": END,\n",
    "        \"incomplete\": \"generate\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKING IF VECTORSTORE IS SUITABLE...\n",
      "what are the permitted activities?\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "ROUTING TO VECTORSTORE\n",
      "RETRIEVING...\n",
      "'Finished running retrieve:'\n",
      "GRADING RELEVANCE...\n",
      "DOCUMENT IS RELEVANT!\n",
      "DOCUMENT IS RELEVANT!\n",
      "DOCUMENT IS RELEVANT!\n",
      "DOCUMENT IS RELEVANT!\n",
      "DETERMINING NEXT NODE...\n",
      "SUFFICIENT INFORMATION! GENERATING ANSWER...\n",
      "'Finished running grade_relevance:'\n",
      "GENERATING...\n",
      "'Finished running generate:'\n",
      "REFLECTING...\n",
      "REFLECTION IS COMPLETE!\n",
      "'Finished running reflect:'\n",
      "('**Permitted Activities in H1 Residential - Large Lot Zone**\\n'\n",
      " '\\n'\n",
      " 'Based on the provided context, the following are the permitted activities in '\n",
      " 'the H1 Residential - Large Lot Zone:\\n'\n",
      " '\\n'\n",
      " '* **Residential**: This is a permitted activity, as indicated by Table '\n",
      " 'H1.4.1 Activity table (A1) and Standard H1.6.4 Building height.\\n'\n",
      " '* **Home occupations**: As per Section H1.6.2, home occupations are allowed '\n",
      " 'in this zone, subject to certain standards being met.\\n'\n",
      " '\\n'\n",
      " '**Standards for Permitted Activities**\\n'\n",
      " '\\n'\n",
      " 'For the permitted activities mentioned above, the following standards must '\n",
      " 'be complied with:\\n'\n",
      " '\\n'\n",
      " '* For residential activity:\\n'\n",
      " '\\t+ Must comply with the standards listed in Table H1.4.1 Activity table '\n",
      " '(Standards to be complied with)\\n'\n",
      " '\\t+ Building height: Standard H1.6.4\\n'\n",
      " '* For home occupations:\\n'\n",
      " '\\t+ At least one person engaged in the home occupation must use the dwelling '\n",
      " 'on the site as their principal place of residence.\\n'\n",
      " '\\t+ No more than two people who do not use the dwelling as their principal '\n",
      " 'place of residence may be employed or work from the dwelling.\\n'\n",
      " '\\n'\n",
      " '**Source**: The information is extracted from the provided documents, '\n",
      " 'specifically:\\n'\n",
      " '\\n'\n",
      " '* Document metadata: '\n",
      " \"'C:\\\\\\\\Users\\\\\\\\GGPC\\\\\\\\Desktop\\\\\\\\Work\\\\\\\\RAG_Local_LLM\\\\\\\\data\\\\\\\\H1 \"\n",
      " \"Residential - Large Lot Zone.pdf'\\n\"\n",
      " '* Page content:\\n'\n",
      " '\\t+ H1.4. Activity table\\n'\n",
      " '\\t+ H1.6. Standards\\n'\n",
      " '\\t+ H1.6.2. Home occupations')\n"
     ]
    }
   ],
   "source": [
    "# Compile \n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "inputs = {\"question\": \"what are the permitted activities?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running {key}:\")\n",
    "pprint(value[\"generation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKING IF VECTORSTORE IS SUITABLE...\n",
      "What is LLM?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "ROUTING TO WEB SEARCH\n",
      "WEB SEARCHING...\n",
      "'Finished running: websearch:'\n",
      "GRADING RELEVANCE...\n",
      "DOCUMENT IS RELEVANT!\n",
      "DETERMINING NEXT NODE...\n",
      "SUFFICIENT INFORMATION! GENERATING ANSWER...\n",
      "'Finished running: grade_relevance:'\n",
      "GENERATING...\n",
      "'Finished running: generate:'\n",
      "REFLECTING...\n",
      "REFLECTION IS COMPLETE!\n",
      "'Finished running: reflect:'\n",
      "('**What is a Large Language Model (LLM)?**\\n'\n",
      " '\\n'\n",
      " 'A Large Language Model (LLM) is a type of machine-learning algorithm that '\n",
      " 'predicts the next word based on input text. It is designed to understand and '\n",
      " 'generate natural language, as well as other types of content, based on the '\n",
      " 'vast amount of data used to train it.\\n'\n",
      " '\\n'\n",
      " '**Key Characteristics:**\\n'\n",
      " '\\n'\n",
      " '* LLMs are trained on immense amounts of data, making them capable of '\n",
      " 'understanding and generating human-like text.\\n'\n",
      " '* They use neural network techniques with extensive parameters for advanced '\n",
      " 'language processing.\\n'\n",
      " '* LLMs can perform a wide range of tasks, including text generation, '\n",
      " 'question answering, and more.\\n'\n",
      " '\\n'\n",
      " '**Source:** The provided context is from an article discussing the '\n",
      " 'capabilities and limitations of Large Language Models (LLMs) in artificial '\n",
      " 'intelligence.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "inputs = {\"question\": \"What is LLM?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
